{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.embeddingLayer = nn.Conv2d(3,512, 16, 16)\n",
    "        self.positionalEncoding = PositionalEncoding(512, max_len=4)\n",
    "        self.transformerEncoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(512, 8, 512, activation=\"gelu\"), 6)\n",
    "        cls_tensor = torch.randn(1,1,512).repeat(10,1,1)\n",
    "        self.cls = nn.Parameter(cls_tensor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddingLayer(x)\n",
    "                \n",
    "        n, c, w, h = x.shape\n",
    "        x = torch.reshape(x, [n, h * w, c])\n",
    "        \n",
    "        x = self.positionalEncoding(x)\n",
    "\n",
    "        x = torch.cat((self.cls, x), 1)\n",
    "        \n",
    "        x = self.transformerEncoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 512])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.rand(10, 3, 32, 32)\n",
    "vision_transformer = VisionTransformer()\n",
    "res = vision_transformer(test)\n",
    "res.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imask",
   "language": "python",
   "name": "imask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
